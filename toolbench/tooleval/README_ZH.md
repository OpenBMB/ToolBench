<div align= "center">
    <h1> ğŸ› ï¸Tool EvalğŸ¤–</h1>
</div>

é€šè¿‡åœ¨ToolBenchä¸Šå¯¹LLaMAè¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬å¾—åˆ°äº†**ToolLLaMA**ã€‚è€ƒè™‘åˆ°äººå·¥è¯„ä¼°éå¸¸è€—æ—¶ï¼Œæˆ‘ä»¬å€Ÿé‰´[AlpacaEval](https://tatsu-lab.github.io/alpaca_eval/)å¼€å‘äº†ä¸€ä¸ªé«˜æ•ˆçš„æœºå™¨è‡ªåŠ¨è¯„ä¼°**ToolEval**ï¼Œå…¶ä¸­åŒ…å«ä¸¤ä¸ªè¯„ä¼°æŒ‡æ ‡ï¼š

- **é€šè¿‡ç‡**ï¼šè®¡ç®—åœ¨æœ‰é™çš„OpenAI APIè°ƒç”¨æ¬¡æ•°å†…æˆåŠŸå®ŒæˆæŒ‡ä»¤çš„æ¯”ä¾‹ã€‚

- **åå¥½**ï¼šé€šè¿‡æ¯”è¾ƒç»™å®šæŒ‡ä»¤çš„ä¸¤ä¸ªç­”æ¡ˆï¼ˆåŠ¨ä½œåºåˆ—ï¼‰æ¥è¡¡é‡ã€‚æˆ‘ä»¬é¢„å…ˆå®šä¹‰äº†ä¸€ç»„æ›´å¥½ç­”æ¡ˆçš„æ ‡å‡†ï¼Œè¿™äº›æ ‡å‡†è¢«ç»„ç»‡æˆChatGPTçš„æç¤ºã€‚æˆ‘ä»¬å‘è¯„ä¼°å™¨æä¾›æµ‹è¯•æŒ‡ä»¤å’Œä¸¤ä¸ªå€™é€‰ç­”æ¡ˆï¼Œå¹¶è·å¾—å…¶åå¥½ã€‚æˆ‘ä»¬å¯¹æ¯ä¸ªç­”æ¡ˆå¯¹è¿›è¡Œå¤šæ¬¡è¯„ä¼°ä»¥æé«˜ç³»ç»Ÿçš„å¯é æ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¡ç®—**ä¼˜èƒœç‡**ï¼ˆè¢«è¯„ä¼°å™¨é€‰æ‹©ä¸ºæ›´ä¼˜çš„ç™¾åˆ†æ¯”ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æˆ‘ä»¬çš„è®ºæ–‡ã€‚

ä¸ºäº†éªŒè¯ChatGPTè¯„ä¼°å™¨åœ¨é€šè¿‡ç‡å’Œèƒœç‡æ–¹é¢çš„å¯é æ€§ï¼Œæˆ‘ä»¬ä»å››ç§ä¸åŒçš„æ–¹æ³•ï¼ˆChatGPT+ReACTï¼ŒChatGPT+DFSDTï¼ŒToolLLaMA+DFSDTå’ŒGPT4+DFSDTï¼‰ä¸­è¿›è¡Œé‡‡æ ·ï¼Œä¸ºæ¯ç§æ–¹æ³•çš„300ä¸ªæµ‹è¯•æŒ‡ä»¤è·å–è§£å†³æ–¹æ¡ˆå¯¹ã€‚ç„¶åï¼Œæˆ‘ä»¬è¯·äººç±»æ ‡æ³¨ChatGPT+DFSDTï¼ŒToolLLaMA+DFSDTå’ŒGPT4+DFSDTçš„é€šè¿‡ç‡ï¼Œä»¥åŠChatGPT+ReACTå’ŒChatGPT+DFSDTä¹‹é—´çš„èƒœç‡ã€‚

æˆ‘ä»¬çš„ChatGPTè¯„ä¼°å™¨åœ¨é€šè¿‡ç‡æ–¹é¢ä¸äººç±»æ ‡æ³¨è€…å…·æœ‰é«˜è¾¾**87.1%**çš„ä¸€è‡´æ€§ï¼Œåœ¨èƒœç‡æ–¹é¢å…·æœ‰**80.3%**çš„ä¸€è‡´æ€§ã€‚è¿™ä¸ªç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è¯„ä¼°å™¨ç”Ÿæˆçš„è¯„ä¼°ç»“æœä¸äººç±»éå¸¸ç›¸ä¼¼ï¼Œå¹¶ä¸”å¯ä»¥è§†ä¸ºåœ¨é€šè¿‡ç‡å’Œèƒœç‡ä¸Šæ¨¡æ‹Ÿäººç±»è¯„ä¼°çš„å¯é è¯„ä¼°å™¨ã€‚
æœ‰å…³ToolEvalçš„æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚é˜…æˆ‘ä»¬çš„è®ºæ–‡ã€‚

## ğŸš€ç”¨æ³•

### Install
Install Package (python>=3.9)
```bash
pip install -r requirements.txt
```

### Evaluation
*è‹¥è¦å¤ç°ç»“æœï¼Œç›´æ¥é€šè¿‡[Google Drive](https://drive.google.com/drive/folders/1yBUQ732mPu-KclJnuQELEhtKakdXFc3J)ä¸‹è½½æˆ‘ä»¬çš„`reproduction_data.zip`ï¼Œè§£å‹åç½®`reproduction_data`äº`ToolBench/data/`ä¸‹å³å¯ï¼Œå¯ä»¥è·³è¿‡æ•°æ®å‡†å¤‡æµç¨‹ã€‚*
- æ•°æ®å‡†å¤‡ã€‚è‹¥è¦ä½¿ç”¨ ToolEval è¯„ä¼°æ‚¨è‡ªå·±çš„æ¨¡å‹å’Œæ–¹æ³•ï¼Œé¦–å…ˆéœ€è¦ä¸ºå…­ä¸ªæµ‹è¯•å­é›†å‡†å¤‡æ‰€æœ‰çš„æ¨¡å‹é¢„æµ‹ã€‚åˆ›å»ºä¸€ä¸ªä»¥æ‚¨çš„æ¨¡å‹å’Œæ–¹æ³•å‘½åçš„ç›®å½•ï¼Œä¾‹å¦‚ `chatgpt_cot`ï¼Œç„¶åå°†æ¯ä¸ªæµ‹è¯•é›†çš„é¢„æµ‹æ”¾åœ¨è¯¥ç›®å½•ä¸‹ã€‚ç›®å½•çš„æ–‡ä»¶ç»“æ„åº”å¦‚ä¸‹ï¼š
```
â”œâ”€â”€ /chatgpt_cot/
â”‚  â”œâ”€â”€ /G1_instruction/
â”‚  â”‚  â”œâ”€â”€ /10160_CoT@1.json
â”‚  â”‚  â””â”€â”€ ...
â”‚  â”œâ”€â”€ /G1_tool/
â”‚  â”‚  â”œâ”€â”€ /10221_CoT@1.json
â”‚  â”‚  â””â”€â”€ ...
â”‚  â”œâ”€â”€ ...
â”‚  â”œâ”€â”€ /G3_instruction/
â”‚  â”‚  â”œâ”€â”€ /10221_CoT@1.json
â”‚  â”‚  â””â”€â”€ ...
```

ç„¶åå¯¹æ¨¡å‹é¢„æµ‹è¿›è¡Œé¢„å¤„ç†:

```bash
export RAW_ANSWER_PATH=../../data/reproduction_data/model_predictions/
export CONVERTED_ANSWER_PATH=../../data/reproduction_data/model_predictions_converted/
export MODEL_NAME=chatgpt_cot
export METHOD=CoT
mkdir ${CONVERTED_ANSWER_PATH}/${MODEL_NAME}
for test_set in G1_instruction G1_category G1_tool G2_category G2_instruction G3_instruction
do
    answer_dir=${RAW_ANSWER_PATH}/${MODEL_NAME}/${test_set}
    output_file=${CONVERTED_ANSWER_PATH}/${MODEL_NAME}/${test_set}.json
    python convert_to_answer_format.py\
        --answer_dir ${answer_dir} \
        --method ${METHOD} \
        --output ${output_file}
done
```
ä¹‹åï¼Œæ£€æŸ¥`${CONVERTED_ANSWER_PATH}/${MODEL_NAME}`ä¸‹æ˜¯å¦æœ‰æµ‹è¯•é›†çš„é¢„å¤„ç†JSONæ–‡ä»¶ã€‚å¦‚æœæœ‰ï¼Œä½ å°±å¯ä»¥å‡†å¤‡è¿è¡Œä»¥ä¸‹è¯„ä¼°è¿‡ç¨‹äº†ã€‚å¦‚æœæ²¡æœ‰ï¼Œè¯·æ£€æŸ¥æ¨¡å‹çš„é¢„æµ‹æ˜¯å¦æœ‰é—®é¢˜ã€‚

- OpenAI Key
å‡†å¤‡æ‚¨çš„OpenAI Keyæ¥æ­å»ºæˆ‘ä»¬çš„evaluatorã€‚Keyéœ€è¦è¢«å­˜å‚¨åˆ°ä¸€ä¸ªjson fileä¸­ï¼Œå¦‚`path/to/your/openai_key_json_file.json`ï¼š
```bash
[
    {
        "username": "your_user_name",
        "passwd": "your_password",
        "api_key": "your_openai_key",
        "organization": "your_organization"
    },
    ...
]
```
- Pass rate.
```bash
export CONVERTED_ANSWER_PATH=../../data/reproduction_data/model_predictions_converted/
export SAVE_PATH=pass_rate_results
export CANDIDATE_MODEL=chatgpt_cot
export API_POOL_FILE=path/to/your/openai_key_json_file.json

python eval_pass_rate.py \
    --converted_answer_path ${CONVERTED_ANSWER_PATH} \
    --save_path ${SAVE_PATH} \
    --reference_model ${CANDIDATE_MODEL} \
    --test_ids ../../data/test_query_ids/ \
    --max_eval_threads 20 \
    --evaluate_times 4

```

ç»“æœæ–‡ä»¶ä¼šè¢«å­˜å‚¨è‡³${SAVE_PATH}ä¸­ã€‚

- Win rate. ä»¥ä¸‹ç¤ºä¾‹ä»¥ChatGPT-ReACTä½œä¸ºå‚è€ƒæ¨¡å‹ï¼ŒGPT4-ReACTä½œä¸ºå€™é€‰æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼Œæ‚¨é¦–å…ˆéœ€è¦è·å–ä¸¤ä¸ªæ¨¡å‹çš„pass rateç»“æœï¼Œç„¶åè¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥è¯„ä¼°GPT4-ReACTçš„win rateç»“æœ:
```bash
export CONVERTED_ANSWER_PATH=../../data/reproduction_data/model_predictions_converted/
export SAVE_PATH=preference_results
export PASS_TARE_PATH=pass_rate_results
export REFERENCE_MODEL=chatgpt_cot
export CANDIDATE_MODEL=gpt-4-0613_cot
export API_POOL_FILE=path/to/your/openai_key_json_file.json

python eval_preference.py \
    --converted_answer_path ${CONVERTED_ANSWER_PATH} \
    --reference_model ${REFERENCE_MODEL} \
    --output_model ${CANDIDATE_MODEL} \
    --test_ids ../../data/test_query_ids/ \
    --save_path ${SAVE_PATH} \
    --pass_rate_result_path ${PASS_TARE_PATH} \
    --max_eval_threads 20 \
    --use_pass_rate true \
    --evaluate_times 4
```

ç»“æœæ–‡ä»¶ä¼šè¢«å­˜å‚¨è‡³${SAVE_PATH}ä¸­ã€‚

### è¯„ä¼°æ–°æ–¹æ³•
è¦è¯„ä¼°é™¤äº†ReACTå’ŒDFSDTä¹‹å¤–çš„æ–¹æ³•ï¼Œæ‚¨éœ€è¦éµå¾ªä»¥ä¸ŠData preparationçš„æ­¥éª¤å‡†å¤‡æ‚¨çš„é¢„å¤„ç†å¥½çš„answeræ•°æ®ã€‚é¢„å¤„ç†å¥½çš„answeræ•°æ®éœ€éµå¾ªä»¥ä¸‹jsonæ ¼å¼:

```json
[
    {
        "method":"method name",
        "total_steps": int, // a integer count total steps in answer details
        "final_answer": "final answer from the method",
        "answer_details":[{
            "role":"node role, can be system, user, assistant and tool",
            "message":"message for the node",
            "next":[//next steps, can have multiple elements if the node have multiple candidates.
                {
                    "role":"",
                    "message":"",
                    "next":[...]
                },
                ...//more candidates
            ]
        }]
    }
    ... // more answers for the give query in the testdata
]
```


### æ›´æ–°æ’è¡Œæ¦œ

å¦‚æœæ‚¨æƒ³å°†æ‚¨çš„æ¨¡å‹çš„ç»“æœä¸Šä¼ åˆ°[ToolEval Leaderboard](https://openbmb.github.io/ToolBench/)ï¼Œè¯·æ‚¨å°†æ‚¨çš„ç»“æœæ–‡ä»¶æ•´ç†æˆä¸Šè¿°æ ¼å¼å‘é€ç»™æˆ‘ä»¬ï¼ˆurtoolbench@gmail.comï¼‰æˆ–è€…å¼€ä¸€ä¸ªpull requestã€‚
æˆ‘ä»¬å°†è¿è¡Œè¯„æµ‹è„šæœ¬æ›´æ–°ç»“æœå¹¶å°†æ‚¨çš„æ¨¡å‹æ·»åŠ åˆ°æ’è¡Œæ¦œä¸­ã€‚


### åˆ›å»ºæ–°çš„è‡ªåŠ¨è¯„ä¼°å™¨
å¦‚æœæ‚¨æƒ³åˆ›å»ºæ–°çš„è‡ªåŠ¨è¯„ä¼°å™¨ï¼Œæ‚¨éœ€è¦æŒ‰ä¸‹åˆ—æ­¥éª¤è¿›è¡Œï¼š
1. åœ¨è·¯å¾„`toolbench/tooleval/evaluators`ä¸‹åˆ›å»ºä¸€ä¸ªè¯„æµ‹å™¨é…ç½®æ–‡ä»¶ç›®å½•ï¼Œå‘½åä¸ä½ çš„è¯„æµ‹å™¨åä¸€è‡´ã€‚åœ¨å…¶ä¸­æ·»åŠ `config.yaml`æ–‡ä»¶ä¸`template.txt`æ–‡ä»¶ã€‚å…·ä½“é…ç½®æ–¹å¼å¯å‚è€ƒ`toolbench/tooleval/evaluators/tooleval_gpt-3.5-turbo_normalized`ä¸­çš„å®ç°ã€‚
2. åˆ›å»ºä½ çš„evaluatorç±»å¹¶å®ç°`fn_completions`å‡½æ•°åœ¨æ–‡ä»¶å¤¹`toolbench/tooleval/evaluators/registered_cls`ä¸­ï¼Œæˆ–è€…ä½ å¯ä»¥ä½¿ç”¨æˆ‘ä»¬é¢„å…ˆå®šä¹‰å¥½çš„ç±»ä¾‹å¦‚`OpenAINormalizedEvaluator`ã€‚
å®Œæˆåå°†é…ç½®æ–‡ä»¶ä¸­`registered_cls_name`å­—æ®µå¡«å†™ä¸ºè¯¥ç±»çš„åç§°ã€‚
è¿™é‡Œç»™å‡ºä¸€ä¸ªä¾‹å­ï¼š
```Python
from evaluators import register_evaluator,BaseEvaluator
from typing import Dict,List

@register_evaluator
class MyEvaluator(BaseEvaluator):
    def __init__(self,config):
        super().__init__(
            fn_completions=self.fn_completions,
        )
        # set your configures here
    
    def fn_completions(self,query:Dict,answers:List[Dict])->int:
        # implement your evaluator here
        # return the index of the preferred answer
        return 0
```
å…¶ä¸­register_evaluatoræ˜¯ä¸€ä¸ªè£…é¥°å™¨ï¼Œç”¨äºæ³¨å†Œè¯„ä¼°å™¨ï¼ŒBaseEvaluatoræ˜¯ä¸€ä¸ªåŸºç±»ï¼Œç”¨äºå®ç°è¯„ä¼°å™¨çš„åŸºæœ¬åŠŸèƒ½ã€‚
3. æµ‹è¯•è¯„ä¼°å™¨çš„æ€§èƒ½ï¼Œè¿è¡Œè„šæœ¬`evaluators_comparison.py`ã€‚
