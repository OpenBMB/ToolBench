<div align= "center">
    <h1> ğŸ› ï¸Tool EvalğŸ¤–</h1>
</div>

é€šè¿‡åœ¨ToolBenchä¸Šå¯¹LLaMAè¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬å¾—åˆ°äº†**ToolLLaMA**ã€‚è€ƒè™‘åˆ°äººå·¥è¯„ä¼°éå¸¸è€—æ—¶ï¼Œæˆ‘ä»¬å€Ÿé‰´[AlpacaEval](https://tatsu-lab.github.io/alpaca_eval/)å¼€å‘äº†ä¸€ä¸ªé«˜æ•ˆçš„æœºå™¨è‡ªåŠ¨è¯„ä¼°**ToolEval**ï¼Œå…¶ä¸­åŒ…å«ä¸¤ä¸ªè¯„ä¼°æŒ‡æ ‡ï¼š

- **é€šè¿‡ç‡**ï¼šè®¡ç®—åœ¨æœ‰é™çš„OpenAI APIè°ƒç”¨æ¬¡æ•°å†…æˆåŠŸå®ŒæˆæŒ‡ä»¤çš„æ¯”ä¾‹ã€‚

- **åå¥½**ï¼šé€šè¿‡æ¯”è¾ƒç»™å®šæŒ‡ä»¤çš„ä¸¤ä¸ªç­”æ¡ˆï¼ˆåŠ¨ä½œåºåˆ—ï¼‰æ¥è¡¡é‡ã€‚æˆ‘ä»¬é¢„å…ˆå®šä¹‰äº†ä¸€ç»„æ›´å¥½ç­”æ¡ˆçš„æ ‡å‡†ï¼Œè¿™äº›æ ‡å‡†è¢«ç»„ç»‡æˆChatGPTçš„æç¤ºã€‚æˆ‘ä»¬å‘è¯„ä¼°å™¨æä¾›æµ‹è¯•æŒ‡ä»¤å’Œä¸¤ä¸ªå€™é€‰ç­”æ¡ˆï¼Œå¹¶è·å¾—å…¶åå¥½ã€‚æˆ‘ä»¬å¯¹æ¯ä¸ªç­”æ¡ˆå¯¹è¿›è¡Œå¤šæ¬¡è¯„ä¼°ä»¥æé«˜ç³»ç»Ÿçš„å¯é æ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¡ç®—**ä¼˜èƒœç‡**ï¼ˆè¢«è¯„ä¼°å™¨é€‰æ‹©ä¸ºæ›´ä¼˜çš„ç™¾åˆ†æ¯”ï¼‰å’Œ**æ ‡å‡†å·®**ï¼ˆä¼˜èƒœç‡çš„æ ‡å‡†è¯¯å·®ï¼‰ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æˆ‘ä»¬çš„è®ºæ–‡ã€‚

ä¸ºäº†éªŒè¯åå¥½æŒ‡æ ‡çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬ä»ä¸‰ç§ä¸åŒæ–¹æ³•ï¼ˆChatGPT+ReACTã€GPT4+ReACTå’ŒChatGPT+DFSDTï¼‰ä¸­éšæœºæŠ½æ ·è·å¾—600ä¸ªæµ‹è¯•æŒ‡ä»¤çš„ç­”æ¡ˆå¯¹ã€‚ç„¶åï¼Œæˆ‘ä»¬é‚€è¯·äººå·¥æ ‡æ³¨äººå‘˜å¯¹å®ƒä»¬è¿›è¡Œäººå·¥åå¥½æ³¨é‡Šï¼ˆæ¯ä¸ªç­”æ¡ˆå¯¹4ä¸ªæ³¨é‡Šï¼Œæ€»å…±2400ä¸ªæ³¨é‡Šï¼‰ã€‚æˆ‘ä»¬ä½¿ç”¨ChatGPTå¼€å‘çš„è‡ªåŠ¨è¯„ä¼°å™¨ä¸äººå·¥æ ‡æ³¨è€…å‘ˆç°å‡ºæ˜¾è‘—çš„**75.8%**ç›¸å…³æ€§ã€‚æˆ‘ä»¬è¿˜è·å¾—äº†ä¸åŒäººå·¥æ ‡æ³¨è€…ä¹‹é—´çš„ä¸€è‡´æ€§ä¸º**83.54%**ï¼Œä¸æˆ‘ä»¬çš„è¯„ä¼°å™¨å’Œäººç±»æ ‡æ³¨è€…ä¹‹é—´çš„ä¸€è‡´æ€§ä¸º**80.21%**ã€‚

æœ‰å…³ToolEvalçš„æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚é˜…æˆ‘ä»¬çš„è®ºæ–‡ã€‚

## ğŸš€ç”¨æ³•

### å®‰è£…
å®‰è£…åŒ…ï¼Œè¦æ±‚(python>=3.9)
```bash
pip install -r requirements.txt
```

### å¤ç°ç»“æœ

è¦åœ¨æµ‹è¯•é›†ï¼ˆå¦‚G1-Inst.ï¼‰ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œå¯ä»¥æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
- é€šè¿‡ç‡:
```bash
python pass_rate.py --answer_dir data/answer/toolllama_dfs/G1_instruction
```
- ä¼˜èƒœç‡ (å‚è€ƒæ¨¡å‹: ChatGPT-ReACT):
```bash
export OPENAI_KEY=""
export REF_MODEL_DATA="data/answer/chatgpt_cot/G1_instruction"
export REF_MODEL_METHOD="CoT"
export TEST_MODEL_DATA="data/answer/toolllama_dfs/G1_instruction"
export TEST_MODEL_METHOD="DFS"
python convert_to_answer_format.py \
    --method CoT \
    --answer_dir ${REF_MODEL_DATA} \
    --output ${REF_MODEL_DATA}_converted

python convert_to_answer_format.py \
    --method DFS \
    --answer_dir ${TEST_MODEL_DATA} \
    --output ${TEST_MODEL_DATA}_converted

python automatic_eval_sample.py \
    --output ${REF_MODEL_DATA}_converted \
    --ref_output ${TEST_MODEL_DATA}_converted \
    --method ${TEST_MODEL_METHOD} \
    --ref_method ${REF_MODEL_METHOD} \
    --use_existed_output
```

### è¯„ä¼°æ–°æ–¹æ³•

é€šè¿‡ç‡çš„è®¡ç®—æ–¹å¼å–å†³äºç‰¹å®šçš„æ–¹æ³•æˆ–æ¨¡å‹ï¼Œè¯·å‚è€ƒæœ‰å…³é€šè¿‡ç‡çš„è¯´æ˜ä¸ä»£ç è®¡ç®—é€šè¿‡ç‡ã€‚
è¿™é‡Œæˆ‘ä»¬ä»‹ç»ä¼˜èƒœç‡çš„è®¡ç®—æ–¹å¼ï¼š

1. å‡†å¤‡å‚è€ƒæ¨¡å‹çš„ç­”æ¡ˆï¼Œæˆ‘ä»¬å°†`ChatGPT-ReACT`æ¨¡å‹åœ¨æˆ‘ä»¬çš„é»˜è®¤æµ‹è¯•é›†ä¸Šçš„ç»“æœä¸Šä¼ åˆ°äº†[Data](https://drive.google.com/drive/folders/1yBUQ732mPu-KclJnuQELEhtKakdXFc3J)ã€‚
è¯·æ³¨æ„ï¼Œä¸‹è½½çš„ç­”æ¡ˆé¦–å…ˆéœ€è¦é€šè¿‡è„šæœ¬`convert_to_answer_format.py`è½¬æ¢ä¸ºè¯„æµ‹ç»“æ„çš„æ ¼å¼ï¼Œå†ç”¨äºè¯„æµ‹ã€‚
2. æŒ‰ä¸‹é¢çš„æ ¼å¼å‡†å¤‡å¾…è¯„æµ‹çš„ç­”æ¡ˆï¼š
```json
[
    {
        "method":"method name",
        "total_steps": int, // ä¸€ä¸ªæ•´æ•°ï¼Œè®°å½•answer_detailsçš„æ€»æ­¥æ•°
        "final_answer": "final answer from the method",
        "answer_details":[{
            "role":"node role, can be system, user, assistant and tool",
            "message":"message for the node",
            "next":[// ä¸‹ä¸€æ­¥ï¼Œå¦‚æœæœ‰å¤šä¸ªå€™é€‰æ­¥ï¼Œå¯ä»¥æœ‰å¤šä¸ªå…ƒç´ 
                {
                    "role":"",
                    "message":"",
                    "next":[...]
                },
                ...//æ›´å¤šå€™é€‰æ­¥
            ]
        }]
    }
    ... // å…¶ä»–ç­”æ¡ˆ
]
```
è¯·æ³¨æ„ï¼Œç­”æ¡ˆçš„é¡ºåºéœ€è¦ä¿æŒå’Œå‚è€ƒæ¨¡å‹çš„ç­”æ¡ˆé¡ºåºä¸€è‡´ã€‚
3. æ‰§è¡Œè¯„æµ‹è„šæœ¬ï¼š
```bash
python automatic_eval_sample.py \
    --output ${REF_MODEL_DATA} \
    --ref_output ${TEST_MODEL_DATA} \
    --method ${TEST_MODEL_METHOD} \
    --ref_method ${REF_MODEL_METHOD} \
    --use_existed_output
```

### æ›´æ–°æ’è¡Œæ¦œ

å¦‚æœæ‚¨æƒ³å°†æ‚¨çš„æ¨¡å‹çš„ç»“æœä¸Šä¼ åˆ°[ToolEval Leaderboard](https://openbmb.github.io/ToolBench/)ï¼Œè¯·æ‚¨å°†æ‚¨çš„ç»“æœæ–‡ä»¶æ•´ç†æˆä¸Šè¿°æ ¼å¼å‘é€ç»™æˆ‘ä»¬ï¼ˆurtoolbench@gmail.comï¼‰ã€‚
æˆ‘ä»¬å°†è¿è¡Œè¯„æµ‹è„šæœ¬æ›´æ–°ç»“æœå¹¶å°†æ‚¨çš„æ¨¡å‹æ·»åŠ åˆ°æ’è¡Œæ¦œä¸­ã€‚

å…·ä½“æ¥è¯´ï¼Œæ‚¨éœ€è¦æä¾›ä¸‹åˆ—ä¿¡æ¯ï¼š
```
Method Name : æ–¹æ³•åç§°
Method Link : æ–¹æ³•é“¾æ¥ï¼Œå¯é€‰
Test Set : (æµ‹è¯•é›†ï¼Œé»˜è®¤ä¸ºdata/test_query_idsä¸­çš„6ä¸ªé›†åˆ)
Comparison Method Name : (å‚è€ƒæ¨¡å‹ï¼Œé»˜è®¤ä¸ºChatGPT-ReACT) 
Answer Files Link : (ç»“æœæ–‡ä»¶ä¸‹è½½é“¾æ¥ï¼Œåº”è¯¥åŒ…å«6ä¸ªjsonæ–‡ä»¶ï¼Œå¯¹åº”æµ‹è¯•é›†ä¸­çš„6ä¸ªå­é›†åˆ)
```

æˆ‘ä»¬å°†è¿è¡Œè„šæœ¬`eval_and_update_leaderboard.py`éªŒè¯ç»“æœå¹¶æ›´æ–°æ’è¡Œæ¦œï¼Œæ‚¨ä¹Ÿå¯ä»¥åœ¨æœ¬åœ°è¿è¡Œè¯¥è„šæœ¬æŸ¥çœ‹ç»“æœã€‚
```bash
python eval_and_update_leaderboard.py \
    --evalset default_evalset \
    --method your_method_name \
    --ref_method ref_method_name \
    --result_folder your_answer_files \
    --ref_result_folder ref_results_files  \
```

## ğŸ”¨è¯„ä¼°è‡ªåŠ¨è¯„ä¼°å™¨
ä¸ºäº†éªŒè¯è‡ªåŠ¨è¯„ä¼°å™¨çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬æ”¶é›†äº†600ä¸ªæµ‹è¯•æŒ‡ä»¤çš„ç­”æ¡ˆå¯¹ï¼Œç„¶åé‚€è¯·äººå·¥æ ‡æ³¨äººå‘˜å¯¹å®ƒä»¬è¿›è¡Œäººå·¥åå¥½æ³¨é‡Šï¼ˆæ¯ä¸ªç­”æ¡ˆå¯¹4ä¸ªæ³¨é‡Šï¼Œæ€»å…±2400ä¸ªæ³¨é‡Šï¼‰ã€‚

### äººç±»äº¤å‰æ ‡æ³¨æ•°æ®é›†
è¯¥æ•°æ®é›†åŒ…å«600ä¸ªç­”æ¡ˆå¯¹ï¼Œæ¯ä¸ªç­”æ¡ˆå¯¹ç”±4ä¸ªäººç±»æ ‡æ³¨è€…æ ‡æ³¨ã€‚
æˆ‘ä»¬éšæœºä»ToolBenchæ•°æ®ä¸­é€‰æ‹©äº†600ä¸ªæµ‹è¯•æŒ‡ä»¤ï¼Œå¹¶éšæœºä»3ä¸ªä¸åŒæ–¹æ³•ï¼ˆChatGPT+ReACTï¼ŒGPT4+ReACT å’Œ ChatGPT+DFSDTï¼‰çš„ç»“æœä¸­æŒ‘é€‰2ä¸ªç­”æ¡ˆç»„æˆç­”æ¡ˆå¯¹ã€‚
ç„¶åæˆ‘ä»¬å¯¹æ¯ä¸ªç­”æ¡ˆé‚€è¯·4ä¸ªäººç±»æ ‡æ³¨è€…è¿›è¡Œåå¥½é€‰æ‹©ï¼Œæ¯ä¸ªæ ‡æ³¨è€…éƒ½ä¼šçœ‹åˆ°ä¸€ä¸ªç­”æ¡ˆå¯¹ï¼Œç„¶åé€‰æ‹©ä»–ä»¬è®¤ä¸ºæ›´å¥½çš„ç­”æ¡ˆã€‚
åŸå§‹æ•°æ®é›†å¯ä»[è¿™é‡Œ](https://drive.google.com/drive/folders/1yBUQ732mPu-KclJnuQELEhtKakdXFc3J)ä¸‹è½½ã€‚

### è‡ªåŠ¨è¯„ä¼°å™¨æ€§èƒ½
æˆ‘ä»¬ä½¿ç”¨äººç±»äº¤å‰æ ‡æ³¨æ•°æ®é›†è¯„ä¼°äº†è‡ªåŠ¨è¯„ä¼°å™¨çš„æ€§èƒ½ã€‚
è¯„æµ‹è„šæœ¬`evaluators_comparison.py`ä¸ºè‡ªåŠ¨è¯„ä¼°å™¨è®¡ç®—äº†4ä¸ªæŒ‡æ ‡ï¼Œå…¶ä¸­ **äººç±»èµåŒç‡**ï¼Œ**åå·®**å’Œ**æ–¹å·®**æ˜¯ä»[AlpacaEval](https://github.com/tatsu-lab/alpaca_eval/tree/main)ä¸­å€Ÿé‰´çš„ã€‚
- **äººç±»èµåŒç‡** ä»£è¡¨äº†å½“å‰è¯„ä¼°è€…å’Œä¸»è¦äººç±»åå¥½çš„ä¸€è‡´æ€§ï¼Œè¶Šé«˜è¶Šä¸€è‡´ã€‚
- **åå·®** è®¡ç®—äº†ä¸»è¦äººç±»åå¥½å’Œä¸»è¦è¯„ä¼°å™¨åå¥½çš„ä¸€è‡´æ€§ï¼Œè¶Šä½ä¸€è‡´æ€§è¶Šé«˜ã€‚
- **æ–¹å·®** è®¡ç®—äº†è¯„ä¼°å™¨çš„ä¸ç¨³å®šæ€§ï¼Œè¶Šä½è¶Šç¨³å®šã€‚
- **ç›¸å…³ç³»æ•°** æ˜¯è‡ªåŠ¨è¯„ä¼°å™¨å’Œäººç±»è¯„ä¼°è€…ä¹‹é—´çš„çš®å°”é€Šç›¸å…³ç³»æ•°ï¼Œè¶Šé«˜è¶Šç›¸å…³ã€‚

è¯„æµ‹ç»“æœå¦‚ä¸‹ï¼š
The result is shown below:
| è¯„ä¼°å™¨                   | äººç±»èµåŒç‡(%) | åå·® | æ–¹å·® | ç›¸å…³ç³»æ•° |
|-------------------------|----------|----------|---------|----------
| Human          | **83.54**   | **0.0**  | 3.97  | N/A   
| tooleval gpt-3.5-turbo normalized           | 80.21       | 19.3       | **3.47**      | **0.7580**       
| tooleval gpt-3.5-turbo fn  | 63.75       | 36.5       | 9.52      | 0.5308       

### åˆ›å»ºæ–°çš„è‡ªåŠ¨è¯„ä¼°å™¨
å¦‚æœæ‚¨æƒ³åˆ›å»ºæ–°çš„è‡ªåŠ¨è¯„ä¼°å™¨ï¼Œæ‚¨éœ€è¦æŒ‰ä¸‹åˆ—æ­¥éª¤è¿›è¡Œï¼š
1. åœ¨è·¯å¾„`toolbench/tooleval/evaluators`ä¸‹åˆ›å»ºä¸€ä¸ªè¯„æµ‹å™¨é…ç½®æ–‡ä»¶ç›®å½•ï¼Œå‘½åä¸ä½ çš„è¯„æµ‹å™¨åä¸€è‡´ã€‚åœ¨å…¶ä¸­æ·»åŠ `config.yaml`æ–‡ä»¶ä¸`template.txt`æ–‡ä»¶ã€‚å…·ä½“é…ç½®æ–¹å¼å¯å‚è€ƒ`toolbench/tooleval/evaluators/tooleval_gpt-3.5-turbo_normalized`ä¸­çš„å®ç°ã€‚
2. åˆ›å»ºä½ çš„evaluatorç±»å¹¶å®ç°`fn_completions`å‡½æ•°åœ¨æ–‡ä»¶å¤¹`toolbench/tooleval/evaluators/registered_cls`ä¸­ï¼Œæˆ–è€…ä½ å¯ä»¥ä½¿ç”¨æˆ‘ä»¬é¢„å…ˆå®šä¹‰å¥½çš„ç±»ä¾‹å¦‚`OpenAINormalizedEvaluator`ã€‚
å®Œæˆåå°†é…ç½®æ–‡ä»¶ä¸­`registered_cls_name`å­—æ®µå¡«å†™ä¸ºè¯¥ç±»çš„åç§°ã€‚
è¿™é‡Œç»™å‡ºä¸€ä¸ªä¾‹å­ï¼š
```Python
from evaluators import register_evaluator,BaseEvaluator
from typing import Dict,List

@register_evaluator
class MyEvaluator(BaseEvaluator):
    def __init__(self,config):
        super().__init__(
            fn_completions=self.fn_completions,
        )
        # set your configures here
    
    def fn_completions(self,query:Dict,answers:List[Dict])->int:
        # implement your evaluator here
        # return the index of the preferred answer
        return 0
```
å…¶ä¸­register_evaluatoræ˜¯ä¸€ä¸ªè£…é¥°å™¨ï¼Œç”¨äºæ³¨å†Œè¯„ä¼°å™¨ï¼ŒBaseEvaluatoræ˜¯ä¸€ä¸ªåŸºç±»ï¼Œç”¨äºå®ç°è¯„ä¼°å™¨çš„åŸºæœ¬åŠŸèƒ½ã€‚
3. æµ‹è¯•è¯„ä¼°å™¨çš„æ€§èƒ½ï¼Œè¿è¡Œè„šæœ¬`evaluators_comparison.py`ã€‚
