{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rrphwMU3wVA"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clone the code from github\n",
        "!git clone https://github.com/OpenBMB/ToolBench.git\n",
        "%cd ToolBench\n",
        "# begin to install\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "ki42Oztl4WRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the dataset from google drive\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Vis-RxBstXLKC1W1agIQUJNuumPJrrw0&confirm=yes' -O data.zip\n",
        "!unzip data.zip"
      ],
      "metadata": {
        "id": "2YlsTn0u4y0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the lora model\n",
        "!mkdir lora\n",
        "!wget https://huggingface.co/ToolBench/ToolLLaMA-7b-LoRA/resolve/main/adapter_model.bin -O ./lora/adapter_model.bin\n",
        "!wget https://huggingface.co/ToolBench/ToolLLaMA-7b-LoRA/resolve/main/adapter_config.json -O ./lora/adapter_config.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izZ1AiifUR9w",
        "outputId": "168ab127-b5bf-404b-eba4-8fd0bd88bfcd"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-07 04:49:26--  https://huggingface.co/ToolBench/ToolLLaMA-7b-LoRA/blob/main/adapter_model.bin\n",
            "Resolving huggingface.co (huggingface.co)... 65.8.49.38, 65.8.49.53, 65.8.49.24, ...\n",
            "Connecting to huggingface.co (huggingface.co)|65.8.49.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42926 (42K) [text/html]\n",
            "Saving to: ‘./lora/adapter_model.bin’\n",
            "\n",
            "\r./lora/adapter_mode   0%[                    ]       0  --.-KB/s               \r./lora/adapter_mode 100%[===================>]  41.92K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-08-07 04:49:26 (4.10 MB/s) - ‘./lora/adapter_model.bin’ saved [42926/42926]\n",
            "\n",
            "--2023-08-07 04:49:26--  https://huggingface.co/ToolBench/ToolLLaMA-7b-LoRA/blob/main/adapter_config.json\n",
            "Resolving huggingface.co (huggingface.co)... 65.8.49.38, 65.8.49.53, 65.8.49.24, ...\n",
            "Connecting to huggingface.co (huggingface.co)|65.8.49.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 48785 (48K) [text/html]\n",
            "Saving to: ‘./lora/adapter_config.json’\n",
            "\n",
            "./lora/adapter_conf 100%[===================>]  47.64K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-08-07 04:49:27 (4.35 MB/s) - ‘./lora/adapter_config.json’ saved [48785/48785]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# you can fill the form to get the toobench_key: https://forms.gle/oCHHc8DQzhGfiT9r6\n",
        "%env TOOLBENCH_KEY=\"put your toolbench key here\""
      ],
      "metadata": {
        "id": "UP8HexOp5KA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# begin to inference with rapidAPI Server for ToolLLaMA-LoRA\n",
        "%env PYTHONPATH=./\n",
        "!python toolbench/inference/qa_pipeline.py \\\n",
        "    --tool_root_dir data/toolenv/tools/ \\\n",
        "    --backbone_model toolllama \\\n",
        "    --model_path huggyllama/llama-7b \\\n",
        "    --lora \\\n",
        "    --lora_path /content/ToolBench/lora/ \\\n",
        "    --max_observation_length 1024 \\\n",
        "    --observ_compress_method truncate \\\n",
        "    --method DFS_woFilter_w2 \\\n",
        "    --input_query_file data/instruction/inference_query_demo.json \\\n",
        "    --output_answer_file data/answer/toolllama_lora_dfs \\\n",
        "    --toolbench_key $TOOLBENCH_KEY\n",
        "\n",
        "#make suere you have enough MEN to load the model or your will failed with an OOM error."
      ],
      "metadata": {
        "id": "TWt501gg5abb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
